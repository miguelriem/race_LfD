  <!--<node name="record" pkg="rosbag" type="record" args="/tf /openni/rgb/image_rect_color /openni/depth_registered/points -O $(find race_learning_by_demonstration)/bags/tmp.bag"/>  -->
<!--    Records raw depth image and RGB image from OpenNI device (kinect).
The depth image is registered with OpenNI firmware, using default calibration.
-->

<launch>

<!--  First we start the openni driver, and all the related
post-processing to get point clouds: actually we don't need all that but in case you want to visualise it in action this is done. For an alternative that does not do that see http://mirror.umd.edu/roswiki/openni_launch(2f)Tutorials(2f)BagRecordingPlayback.html - just change the following line for the appropriate openni_camera node with depth registration switched on, and the rgb and depth frame ids set -->

<include file="$(find openni_launch)/launch/openni.launch" >
<arg name="load_driver" value="true" />
<arg name="depth_registration" value="true"/>
</include>


	<node pkg="openni_tracker" type="openni_tracker" name="openni_tracker" args="">
	<param name="camera_frame_id" type="string" value="/camera_depth_frame" />
	</node>

<!-- Now we record the raw parts to a bag. The
bag will be put in your ROS_WORKSPACE, with the openni_record prefix. This is done because I always set the name manually after inspecting the data. To set an exact name rewrite this to use an argument. -->

<!--<node pkg="rosbag" type="record" name="record" output="screen" args="-O $(find race_learning_by_demonstration)/bags/auto.bag /openni/depth_registered/camera_info /openni/depth_registered/image_raw /openni/rgb/camera_info /openni/rgb/image_raw /tf"/> -->

<node pkg="rosbag" type="record" name="record" output="screen" args="-O $(find race_learning_by_demonstration)/bags/auto.bag /camera/depth_registered/camera_info /camera/depth_registered/image_raw /camera/rgb/camera_info /camera/rgb/image_raw /tf"/> 

<!--<include file="$(find race_learning_by_demonstration)/launch/visualize.launch" />-->
</launch>
